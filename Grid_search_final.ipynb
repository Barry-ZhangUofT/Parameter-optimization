{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Barry-ZhangUofT/Parameter-optimization/blob/main/Grid_search_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Function"
      ],
      "metadata": {
        "id": "9k5AVhAeO0t7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SvAuxRulOvhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4723d6c-372b-41ad-df21-dc6038b25d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-22 18:39:11--  https://raw.githubusercontent.com/krasserm/bayesian-machine-learning/master/bayesian_optimization_util.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1536 (1.5K) [text/plain]\n",
            "Saving to: ‘bayesian_optimization_util.py.1’\n",
            "\n",
            "\r          bayesian_   0%[                    ]       0  --.-KB/s               \rbayesian_optimizati 100%[===================>]   1.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-22 18:39:11 (24.3 MB/s) - ‘bayesian_optimization_util.py.1’ saved [1536/1536]\n",
            "\n",
            "Requirement already satisfied: scikit-optimize==0.5.2 in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.5.2) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.5.2) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize==0.5.2) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.5.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.5.2) (3.3.0)\n",
            "Requirement already satisfied: GPy==1.9.8 in /usr/local/lib/python3.10/dist-packages (1.9.8)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.10/dist-packages (from GPy==1.9.8) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.10/dist-packages (from GPy==1.9.8) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from GPy==1.9.8) (1.16.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from GPy==1.9.8) (0.9.6)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.10/dist-packages (from paramz>=0.9.0->GPy==1.9.8) (4.4.2)\n",
            "Collecting GPyOpt==1.2.1\n",
            "  Using cached GPyOpt-1.2.1.tar.gz (46 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: xgboost==0.90 in /usr/local/lib/python3.10/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost==0.90) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost==0.90) (1.11.4)\n",
            "Collecting scipy==1.4.1\n",
            "  Using cached scipy-1.4.1.tar.gz (24.6 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Check if notebook is running in Google Colab\n",
        "    import google.colab\n",
        "    # Get additional files from Github\n",
        "    !wget https://raw.githubusercontent.com/krasserm/bayesian-machine-learning/master/bayesian_optimization_util.py\n",
        "    # Install additional dependencies\n",
        "    !pip install scikit-optimize==0.5.2\n",
        "    !pip install GPy==1.9.8\n",
        "    !pip install GPyOpt==1.2.1\n",
        "    !pip install xgboost==0.90\n",
        "    !pip install --upgrade scipy==1.4.1\n",
        "\n",
        "\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9WReyuPc42c",
        "outputId": "6939de8d-5925-4783-8573-5931f0a2f166"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "from bayesian_optimization_util import plot_approximation, plot_acquisition\n",
        "from sklearn.gaussian_process.kernels import RBF,ConstantKernel as C,RationalQuadratic as RQ,WhiteKernel as WK,ExpSineSquared as Exp,DotProduct as Lin\n",
        "from numpy import exp,arange\n",
        "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show\n",
        "\n",
        "from mlxtend.preprocessing import standardize,minmax_scaling\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RepeatedKFold\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from operator import add\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_log_error, r2_score, median_absolute_error\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "from threading import activeCount\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "import sklearn.metrics\n",
        "import math\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5aFNrsOKO67Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
        "    '''\n",
        "    Computes the expected_improvement at points X based on existing samples X_sample\n",
        "    and Y_sample using a Gaussian process surrogate model.\n",
        "\n",
        "    Args:\n",
        "        X: Points at which expected_improvement shall be computed (m x d).\n",
        "        X_sample: Sample locations (n x d).\n",
        "        Y_sample: Sample values (n x 1).\n",
        "        gpr: A GaussianProcessRegressor fitted to samples.\n",
        "        xi: Exploitation-exploration trade-off parameter.\n",
        "\n",
        "    Returns:\n",
        "        Expected improvements at points X.\n",
        "    '''\n",
        "    mu, sigma = gpr.predict(X, return_std=True)\n",
        "    mu_sample = gpr.predict(X_sample)\n",
        "\n",
        "    sigma = sigma.reshape(-1, 1)\n",
        "\n",
        "    # Needed for noise-based model,\n",
        "    # otherwise use np.max(Y_sample).\n",
        "    # See also section 2.4 in [...]\n",
        "    mu_sample_opt = np.max(mu_sample)\n",
        "\n",
        "    with np.errstate(divide='warn'):\n",
        "        imp = mu - mu_sample_opt - xi\n",
        "        Z = imp / sigma\n",
        "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
        "        ei[sigma == 0.0] = 0.0\n",
        "\n",
        "    return ei\n",
        "\n",
        "def Upper_boundary(X, X_sample, Y_sample, gpr, sigma):\n",
        "\n",
        "    output1, output2 = gpr.predict(X, return_std=True)\n",
        "    output1 = output1.flatten()\n",
        "    prediction = np.maximum(output1,output2)\n",
        "    std = np.minimum(output1,output2)\n",
        "\n",
        "\n",
        "    upper_bound_value = np.add(prediction, sigma*std)\n",
        "\n",
        "    return upper_bound_value\n",
        "\n",
        "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, sigma = 1, n_restarts=25):\n",
        "    '''\n",
        "    Proposes the next sampling point by optimizing the acquisition function.\n",
        "\n",
        "    Args:\n",
        "        acquisition: Acquisition function.\n",
        "        X_sample: Sample locations (n x d).\n",
        "        Y_sample: Sample values (n x 1).\n",
        "        gpr: A GaussianProcessRegressor fitted to samples.\n",
        "\n",
        "    Returns:\n",
        "        Location of the acquisition function maximum.\n",
        "    '''\n",
        "    dim = X_sample.shape[1]\n",
        "    min_val = 1\n",
        "    min_x = None\n",
        "\n",
        "    def min_obj(X):\n",
        "        # Minimization objective is the negative acquisition function\n",
        "\n",
        "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr, sigma)\n",
        "\n",
        "    # Find the best optimum by starting from n_restart different random points.\n",
        "    for x0 in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
        "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')\n",
        "        if res.fun < min_val:\n",
        "          min_val = res.fun\n",
        "            #min_val = res.fun[0]\n",
        "          min_x = res.x\n",
        "\n",
        "    return min_x.reshape(-1, 1)\n",
        "\n",
        "def distance(p1, p2):\n",
        "    return math.sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
        "def closest(pt, others):\n",
        "  # return closest point to pt in others\n",
        "    return min(others, key = lambda i: distance(pt, i))\n",
        "\n",
        "def denormalize(X_std):\n",
        "    ratio = [3, 20]\n",
        "    min = [1, 15]\n",
        "    X_original = X_std * ratio + min\n",
        "    return X_original\n",
        "def Normalize(X):\n",
        "    ratio = [3, 20]\n",
        "    min = [1, 15]\n",
        "    X_nml = (X - min) / ratio\n",
        "    return X_nml\n",
        "\n",
        "def predict(gpr, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, X_normalized = True, All_samples_at_once = True, Acquisition_function = Upper_boundary):\n",
        "\n",
        "    if i == 0:\n",
        "        prediction = []\n",
        "        # Normalize\n",
        "        if X_normalized:\n",
        "            if len(X_init) != 0:\n",
        "                X_init = Normalize(X_init)\n",
        "            if len(X_pool) != 0:\n",
        "                X_pool = Normalize(X_pool)\n",
        "        # Put all samples to init\n",
        "        if All_samples_at_once:\n",
        "            X_init = np.vstack((X_init, X_pool))\n",
        "            Y_init = np.vstack((Y_init, Y_pool))\n",
        "\n",
        "\n",
        "        # Initialize samples\n",
        "        global X_sample\n",
        "        X_sample = X_init\n",
        "        global Y_sample\n",
        "        Y_sample = Y_init\n",
        "\n",
        "        plt.figure(figsize=(12, 10 * 3))\n",
        "        plt.subplots_adjust(hspace=0.4)\n",
        "\n",
        "\n",
        "        if X_normalized:\n",
        "            bounds = np.array([[0,1],[0,1]])\n",
        "        else:\n",
        "            bounds = np.array([[1,4],[15,35]])\n",
        "\n",
        "    # Update Gaussian process with existing samples\n",
        "    gpr.fit(X_sample, Y_sample)\n",
        "\n",
        "\n",
        "    xi = 0.01 #调这儿来改expected_improvement的参数\n",
        "    sigma = 10 #调这儿来改Upper_boundary的参数 0 1 10 均值+std*sigma 会改变std权重\n",
        "\n",
        "\n",
        "    # Two methods to propose next sample location:\n",
        "    # Find the sample that is closest to proposed location\n",
        "    if Acquisition_function == Upper_boundary:\n",
        "        X_proposal = propose_location(Upper_boundary, X_sample, Y_sample, gpr, bounds, sigma = sigma).flatten()\n",
        "    if Acquisition_function == expected_improvement:\n",
        "        X_proposal = propose_location(expected_improvement, X_sample, Y_sample, gpr, bounds, xi).flatten()\n",
        "\n",
        "    X_next_availble_closest_to_proposeal = closest(X_proposal, X_pool)\n",
        "\n",
        "    # Find the sample in pool with the highest upper_bound_value\n",
        "    if Acquisition_function == Upper_boundary:\n",
        "        aquisition_function_value = Upper_boundary(X_pool, X_sample, Y_sample, gpr, sigma = sigma)\n",
        "        index = np.where(aquisition_function_value == np.amax(aquisition_function_value))\n",
        "        X_next_availble_with_highest_aquisition_function_value = X_pool[index][0].reshape(1,2)[0]\n",
        "    if Acquisition_function == expected_improvement:\n",
        "        aquisition_function_value = expected_improvement(X_pool, X_sample, Y_sample, gpr, xi)\n",
        "        index = np.where(aquisition_function_value == np.amax(aquisition_function_value))\n",
        "        X_next_availble_with_highest_aquisition_function_value = X_pool[index][0]\n",
        "    if (Acquisition_function != Upper_boundary) & (Acquisition_function != expected_improvement):\n",
        "        print('WARNING: Acquisition_function not sepcified!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "\n",
        "\n",
        "    # Choose which method to use:\n",
        "    X_next_availble = X_next_availble_closest_to_proposeal#调这儿来改用那个method找pool里的下一个最高点。 X_next_availble_closest_to_proposeal or X_next_availble_with_highest_aquisition_function_value\n",
        "\n",
        "\n",
        "\n",
        "    # Obtain Y_next_availble using X_next_availble\n",
        "    t1 = np.argwhere(X_pool[:,0] == X_next_availble[0])\n",
        "    t2 = np.argwhere(X_pool[:,1] == X_next_availble[1])\n",
        "    index = np.intersect1d(t1,t2)\n",
        "    Y_next_availble = Y_pool[index]\n",
        "\n",
        "    # Record prediction_history\n",
        "    prediction_next,std1 = gpr.predict(X_proposal.reshape(1,2),return_std=True)\n",
        "    prediction,std = gpr.predict(X_next_availble.reshape(1,2),return_std=True)\n",
        "    print('prediction_next is',prediction_next,'std1 is',std1)\n",
        "    print('prediction is',prediction,'std1 is',std)\n",
        "    prediction_history.append(float(prediction))\n",
        "    std_history.append(float(std))\n",
        "\n",
        "\n",
        "    #Denormalize to original datas for plotting\n",
        "    if X_normalized:\n",
        "        X_sample_dn = denormalize(X_sample)\n",
        "        X_next_availble_dn = denormalize(X_next_availble)\n",
        "        X_proposal_dn = denormalize(X_proposal)\n",
        "    else:\n",
        "        X_sample_dn = X_sample\n",
        "        X_next_availble_dn = X_next_availble\n",
        "        X_proposal_dn = X_proposal\n",
        "\n",
        "\n",
        "    print(\"the next proposed location is {}，predicted to be {},std to be {}\".format(X_proposal_dn, prediction_next,std1))\n",
        "    print(\" \")\n",
        "    print(\"the next sample going to be used is {}, predicted to be {},std to be {}\".format(X_next_availble_dn, prediction,std))\n",
        "    print(\" \")\n",
        "    print(\"Predition history: \")\n",
        "    print(prediction_history)\n",
        "    print(\" \")\n",
        "    print(\"std history: \")\n",
        "    print(std_history)\n",
        "    print(\" \")\n",
        "    print(\"X's used: \")\n",
        "    print(X_sample_dn)\n",
        "    print(\" \")\n",
        "    print(\"Y's used: \")\n",
        "    print(Y_sample)\n",
        "    print(\" \")\n",
        "    print(\"xi is {}\".format(xi))\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "    # Plot 3D pridictions\n",
        "    fig = plt.figure(figsize=(10,6))\n",
        "    ax1 = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    x1 = np.arange(0.9,4.2,0.1)\n",
        "    x2 = np.arange(14,36,0.5)\n",
        "    X1,X2 = np.meshgrid(x1,x2)\n",
        "\n",
        "    test1 = np.reshape(X1,(1452,1))\n",
        "    test2 = np.reshape(X2,(1452,1))\n",
        "    X12 = np.concatenate((test1, test2), axis=1)\n",
        "    if X_normalized:\n",
        "        Y = gpr.predict(Normalize(X12))\n",
        "    else:\n",
        "        Y = gpr.predict(X12)\n",
        "    Y = np.reshape(Y,(44, 33))\n",
        "\n",
        "\n",
        "    mycmap = plt.get_cmap('gist_earth')\n",
        "    ax1.set_title('Iteration {}'.format(i))\n",
        "    surf1 = ax1.plot_surface(X1, X2, Y, cmap=mycmap)\n",
        "    ax1.scatter(X_sample_dn[:,0], X_sample_dn[:,1], Y_sample[:], color = 'black')\n",
        "    ax1.scatter(X_proposal_dn[0], X_proposal_dn[1], Y_sample[-1], color='yellow')\n",
        "    ax1.scatter(X_next_availble_dn[0], X_next_availble_dn[1], Y_sample[-1], color='red')\n",
        "    fig.colorbar(surf1, ax=ax1, shrink=0.5, aspect=5)\n",
        "    plt.show()\n",
        "\n",
        "    plot = plt.pcolormesh(X1, X2, Y, cmap='RdBu', shading='auto')\n",
        "\n",
        "    # Plot 2D pridictions\n",
        "    cset = plt.contour(X1, X2, Y, cmap='gray', levels = np.arange(95, 103, 0.2))#改contour的levels\n",
        "    plt.clabel(cset, inline=True)\n",
        "    plt.plot(X_sample_dn[:,0], X_sample_dn[:,1], 'o', color='black')\n",
        "    plt.plot(X_proposal_dn[0], X_proposal_dn[1], 'o', color='yellow')\n",
        "    plt.plot(X_next_availble_dn[0], X_next_availble_dn[1], 'o', color='red')\n",
        "    plt.colorbar(plot)\n",
        "    plt.show()\n",
        "\n",
        "    # Add new sample to previous samples\n",
        "    X_sample = np.vstack((X_sample, X_next_availble))\n",
        "    Y_sample = np.vstack((Y_sample, Y_next_availble))\n",
        "\n",
        "    # Remove used sample\n",
        "    index_to_remove = np.where(np.all(X_pool == X_next_availble,axis=1))\n",
        "    X_pool = np.delete(X_pool, index_to_remove, 0)\n",
        "    Y_pool = np.delete(Y_pool, index_to_remove, 0)\n",
        "\n",
        "    i +=1\n",
        "\n",
        "    print('i is',i)\n",
        "\n",
        "    return gpr, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, denormalize(X_sample)\n",
        "\n",
        "def rmse(y_test,y_prediction):\n",
        "  MSE = sklearn.metrics.mean_squared_error(y_test,y_prediction)\n",
        "  RMSE = math.sqrt(MSE)\n",
        "  return RMSE\n"
      ],
      "metadata": {
        "id": "6W_Bib1DPn9c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Setup"
      ],
      "metadata": {
        "id": "cbnIdydPP4fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_6 = np.array([[1,15], [1,20], [1,25], [1,30], [1,35],\n",
        "                  [2,15], [2,20], [2,25], [2,30], [2,35],\n",
        "                  [3,15], [3,20], [3,25], [3,30], [3,35],\n",
        "                  [4,15], [4,20], [4,25], [4,30], [4,35],\n",
        "                  [1,15], [1,20], [1,25], [1,30], [1,35],\n",
        "                  [2,15], [2,20], [2,25], [2,30], [2,35],\n",
        "                  [3,15], [3,20], [3,25], [3,30], [3,35],\n",
        "                  [4,15], [4,20], [4,25], [4,30], [4,35]])\n",
        "y_train_6 = np.array([[99.39], [98.603], [97.8], [97.14], [96.65],\n",
        "                  [99.54], [99.8], [99.94], [99.85], [99.93],\n",
        "                  [98.62], [99.27], [99.26], [99.67], [99.51],\n",
        "                  [97.75], [98.82], [99.07], [99.04], [99.15],\n",
        "                  [96.77], [97.91], [98], [97.31], [97.41],\n",
        "                  [99.63], [99.87], [99.89], [99.81], [99.82],\n",
        "                  [99.17], [98.97], [99.72], [99.73], [99.82],\n",
        "                  [99.02], [99.47], [99.71], [99.41], [99.55]])\n",
        "\n",
        "std_train = np.array([[1.31], [0.3465], [0.1], [0.085], [0.38],\n",
        "            [0.045], [0.035], [0.025], [0.02], [0.055],\n",
        "            [0.275], [0.15], [0.23], [0.03], [0.155],\n",
        "            [0.635], [0.325], [0.32], [0.185], [0.2]])\n",
        "\n",
        "X_test_6 = np.array([[1.3,27.5], [2,17.5], [2.7,27.5], [2.5,35], [2.5,20], [1.5,35], [1.5,20], [2,27.5]])\n",
        "y_test_6 = np.array([[99.045], [99.53], [99.54], [99.775], [99.535], [99.425], [99.69], [99.78]]) #Validation\n",
        "std_test = np.array([[0.035], [0.13], [0.28], [0.105], [0.255], [0.105], [0.07], [0.1]])\n",
        "\n",
        "X_normalized = True"
      ],
      "metadata": {
        "id": "uxtaI6MnP9Ac"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D RQ with alpha"
      ],
      "metadata": {
        "id": "ZYJo-eqtFT-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1yn-ecq0FaEX"
      },
      "outputs": [],
      "source": [
        "def change_model_1d_length_scale_alpha(X_train, X_test, y_train, y_test, std_test, alpha, length_scale, X_normalized, test_results, eval_metric):\n",
        "    # initialize\n",
        "    i = 0\n",
        "    prediction_history = []\n",
        "    std_history = []\n",
        "    X_next = []\n",
        "    X_next_availble = []\n",
        "\n",
        "    if X_normalized:\n",
        "        bounds = np.array([[0,1],[0,1]])     #True\n",
        "    else:\n",
        "        bounds = np.array([[1,4],[15,35]])    #False\n",
        "\n",
        "\n",
        "    kernel = 1 * RQ(length_scale, alpha = alpha) # Change nu here if using Matern\n",
        "\n",
        "    # GPR\n",
        "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=9)\n",
        "    Acquisition_function = Upper_boundary #Change Acquisition_function between Upper_boundary and expected_improvement\n",
        "\n",
        "    # Train\n",
        "    model, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, X_sample_dn = predict(gpr, prediction_history, std_history, i, X_train[0:4], y_train[0:4], X_train[4:], y_train[4:], X_next, X_next_availble, bounds, X_normalized=X_normalized, All_samples_at_once = 1, Acquisition_function = Acquisition_function)\n",
        "\n",
        "    # Predict\n",
        "    if X_normalized:\n",
        "        X_test = Normalize(X_test)\n",
        "    y_result = gpr.predict(X_test)\n",
        "    output1, output2 = gpr.predict(X_test, return_std=True)\n",
        "    output1 = output1.flatten()\n",
        "    y_prediction = np.maximum(output1,output2)\n",
        "    y_std = np.minimum(output1,output2)\n",
        "\n",
        "\n",
        "    test_results[0]=(X_test)\n",
        "    test_results[1]=(y_prediction)\n",
        "    test_results[2]=(y_std)\n",
        "    print('test_results')\n",
        "    print(test_results)\n",
        "\n",
        "    #Error Calculation\n",
        "    if eval_metric == 'rmse':\n",
        "      error_value_pred = rmse(y_test,y_prediction)\n",
        "      error_value_std = rmse(std_test,y_std)\n",
        "      likelihood_value = 0\n",
        "    if eval_metric == 'likelihood':\n",
        "      error_value_pred = 0\n",
        "      error_value_std = 0\n",
        "      likelihood_value = model.log_marginal_likelihood()\n",
        "\n",
        "    return error_value_pred, error_value_std, likelihood_value, y_std, y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZadtYJJPFaEY"
      },
      "outputs": [],
      "source": [
        "def grid_search_1d_length_scale_alpha(eval_metric, X_normalized, length_scale_start = 0.1, length_scale_end = 1.1, length_scale_step = 0.1, alpha_start = 1, alpha_end = 10, alpha_step = 1):\n",
        "    graph_x = []\n",
        "    graph_y = []\n",
        "    graph_pred = []\n",
        "    graph_std = []\n",
        "    graph_likelihood = []\n",
        "    test_results = [[],[],[]]\n",
        "\n",
        "    for length_scale in np.arange(length_scale_start, length_scale_end, length_scale_step):#调这儿来改grid search的range\n",
        "        graph_x_row = []\n",
        "        graph_y_row = []\n",
        "        graph_z_pred = []\n",
        "        graph_z_std = []\n",
        "        graph_z_likelihood = []\n",
        "\n",
        "        for alpha in np.arange(alpha_start, alpha_end, alpha_step):\n",
        "            print('Running %.4f %.4f' %(length_scale, alpha))\n",
        "            error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_1d_length_scale_alpha(X_train_6, X_test_6, y_train_6, y_test_6, std_test, alpha, length_scale, X_normalized, test_results, eval_metric)\n",
        "\n",
        "            if eval_metric == 'likelihood':\n",
        "                print('length_scale is', length_scale, 'alpha is', alpha, 'likelihood_value is', likelihood_value)\n",
        "            else:\n",
        "                print('length_scale is', length_scale, 'alpha is', alpha, 'error_value_pred is', error_value_pred, 'error_value_std is', error_value_std)\n",
        "\n",
        "            graph_x_row.append(length_scale)\n",
        "            graph_y_row.append(alpha)\n",
        "            graph_z_pred.append(error_value_pred)\n",
        "            graph_z_std.append(error_value_std)\n",
        "            graph_z_likelihood.append(likelihood_value)\n",
        "\n",
        "        graph_x.append(graph_x_row)\n",
        "        graph_y.append(graph_y_row)\n",
        "        graph_pred.append(graph_z_pred)\n",
        "        graph_std.append(graph_z_std)\n",
        "        graph_likelihood.append(graph_z_likelihood)\n",
        "        print('')\n",
        "\n",
        "    graph_x = np.array(graph_x)\n",
        "    graph_y = np.array(graph_y)\n",
        "    graph_pred = np.array(graph_pred)\n",
        "    graph_std = np.array(graph_std)\n",
        "    graph_likelihood = np.array(graph_likelihood)\n",
        "\n",
        "    min_pred = np.min(graph_pred)\n",
        "    min_std = np.min(graph_std)\n",
        "    max_likelihood = np.max(graph_likelihood)\n",
        "\n",
        "    pos_min_pred = np.argwhere(graph_pred == np.min(graph_pred))\n",
        "    pos_min_std = np.argwhere(graph_std == np.min(graph_std))\n",
        "    pos_max_likelihood = np.argwhere(graph_likelihood == np.max(graph_likelihood))\n",
        "\n",
        "    graph_x_f = graph_x.flatten()\n",
        "    graph_y_f = graph_y.flatten()\n",
        "    graph_pred_f = graph_pred.flatten()\n",
        "    graph_std_f = graph_std.flatten()\n",
        "    graph_likelihood_f = graph_likelihood.flatten()\n",
        "\n",
        "    graph_pred_f = minmax_scale(graph_pred_f,(5,30))\n",
        "    graph_std_f = minmax_scale(graph_std_f,(5,30))\n",
        "    graph_likelihood_f = minmax_scale(graph_std_f,(5,30))\n",
        "\n",
        "    if eval_metric == 'likelihood':\n",
        "        for i in range(len(graph_likelihood_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_likelihood_f[i], color='orange')\n",
        "\n",
        "        plt.plot(graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], graph_y[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('likelihood w.r.t. different length scale and alpha')\n",
        "        plt.show()\n",
        "        print('With likelihood as metric, the maximum likelihood value is', max_likelihood, 'when kernel is RQ with 1d length scale of', graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'and alpha of', graph_y[pos_max_likelihood[0][0],pos_max_likelihood[0][1]])\n",
        "\n",
        "    if eval_metric == 'rmse':\n",
        "        for i in range(len(graph_pred_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_pred_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], graph_y[pos_min_pred[0][0],pos_min_pred[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale and alpha on prediction')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on prediction is', min_pred, 'when kernel is RQ with 1d length scale of', graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], 'and alpha of', graph_y[pos_min_pred[0][0],pos_min_pred[0][1]])\n",
        "\n",
        "\n",
        "        for i in range(len(graph_std_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_std_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_std[0][0],pos_min_std[0][1]], graph_y[pos_min_std[0][0],pos_min_std[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale and alpha on std')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on std is', min_std, 'when kernel is RQ with 1d length scale of', graph_x[pos_min_std[0][0],pos_min_std[0][1]], 'and alpha of', graph_y[pos_min_std[0][0],pos_min_std[0][1]])\n",
        "\n",
        "\n",
        "\n",
        "    #return graph_x,graph_y,graph_pred,graph_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#              rmse,likelihood\n",
        "grid_search_1d_length_scale_alpha('rmse', X_normalized, 0.1, 1.1, 0.1, 1, 10, 1)"
      ],
      "metadata": {
        "id": "WC6U-JQ-GPcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D Matern with nu"
      ],
      "metadata": {
        "id": "2yjF1oHEaoxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wgSTd1zzawCM"
      },
      "outputs": [],
      "source": [
        "def change_model_1d_length_scale_nu(X_train, X_test, y_train, y_test, std_test, nu, length_scale, X_normalized, test_results, eval_metric):\n",
        "    # initialize\n",
        "    i = 0\n",
        "    prediction_history = []\n",
        "    std_history = []\n",
        "    X_next = []\n",
        "    X_next_availble = []\n",
        "\n",
        "    if X_normalized:\n",
        "        bounds = np.array([[0,1],[0,1]])     #True\n",
        "    else:\n",
        "        bounds = np.array([[1,4],[15,35]])    #False\n",
        "\n",
        "\n",
        "    kernel = 1 * Matern(length_scale = length_scale, nu = nu) # Change nu here if using Matern\n",
        "\n",
        "    # GPR\n",
        "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=9)\n",
        "    Acquisition_function = Upper_boundary #Change Acquisition_function between Upper_boundary and expected_improvement\n",
        "\n",
        "    # Train\n",
        "    model, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, X_sample_dn = predict(gpr, prediction_history, std_history, i, X_train[0:4], y_train[0:4], X_train[4:], y_train[4:], X_next, X_next_availble, bounds, X_normalized=X_normalized, All_samples_at_once = 1, Acquisition_function = Acquisition_function)\n",
        "\n",
        "    # Predict\n",
        "    if X_normalized:\n",
        "        X_test = Normalize(X_test)\n",
        "    y_result = gpr.predict(X_test)\n",
        "    output1, output2 = gpr.predict(X_test, return_std=True)\n",
        "    output1 = output1.flatten()\n",
        "    y_prediction = np.maximum(output1,output2)\n",
        "    y_std = np.minimum(output1,output2)\n",
        "\n",
        "\n",
        "    test_results[0]=(X_test)\n",
        "    test_results[1]=(y_prediction)\n",
        "    test_results[2]=(y_std)\n",
        "    print('test_results')\n",
        "    print(test_results)\n",
        "\n",
        "    #Error Calculation\n",
        "    if eval_metric == 'rmse':\n",
        "      error_value_pred = rmse(y_test,y_prediction)\n",
        "      error_value_std = rmse(std_test,y_std)\n",
        "      likelihood_value = 0\n",
        "    if eval_metric == 'likelihood':\n",
        "      error_value_pred = 0\n",
        "      error_value_std = 0\n",
        "      likelihood_value = model.log_marginal_likelihood()\n",
        "\n",
        "    return error_value_pred, error_value_std, likelihood_value, y_std, y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fMWZt5HIbbjD"
      },
      "outputs": [],
      "source": [
        "def grid_search_1d_length_scale_nu(eval_metric, X_normalized, length_scale_start = 0.1, length_scale_end = 1.1, length_scale_step = 0.1):\n",
        "    graph_x = []\n",
        "    graph_y = []\n",
        "    graph_pred = []\n",
        "    graph_std = []\n",
        "    graph_likelihood = []\n",
        "    test_results = [[],[],[]]\n",
        "    nu_list = list((0.5, 1.5, 2.5, float('inf')))\n",
        "\n",
        "    for length_scale in np.arange(length_scale_start, length_scale_end, length_scale_step):#调这儿来改grid search的range\n",
        "        graph_x_row = []\n",
        "        graph_y_row = []\n",
        "        graph_z_pred = []\n",
        "        graph_z_std = []\n",
        "        graph_z_likelihood = []\n",
        "\n",
        "        for nu in nu_list:\n",
        "            print('Running %.4f %.4f' %(length_scale, nu))\n",
        "            error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_1d_length_scale_nu(X_train_6, X_test_6, y_train_6, y_test_6, std_test, nu, length_scale, X_normalized, test_results, eval_metric)\n",
        "\n",
        "            if eval_metric == 'likelihood':\n",
        "                print('length_scale is', length_scale, 'nu is', nu, 'likelihood_value is', likelihood_value)\n",
        "            else:\n",
        "                print('length_scale is', length_scale, 'nu is', nu, 'error_value_pred is', error_value_pred, 'error_value_std is', error_value_std)\n",
        "\n",
        "            graph_x_row.append(length_scale)\n",
        "            graph_y_row.append(nu)\n",
        "            graph_z_pred.append(error_value_pred)\n",
        "            graph_z_std.append(error_value_std)\n",
        "            graph_z_likelihood.append(likelihood_value)\n",
        "\n",
        "        graph_x.append(graph_x_row)\n",
        "        graph_y.append(graph_y_row)\n",
        "        graph_pred.append(graph_z_pred)\n",
        "        graph_std.append(graph_z_std)\n",
        "        graph_likelihood.append(graph_z_likelihood)\n",
        "        print('')\n",
        "\n",
        "    graph_x = np.array(graph_x)\n",
        "    graph_y = np.array(graph_y)\n",
        "    graph_pred = np.array(graph_pred)\n",
        "    graph_std = np.array(graph_std)\n",
        "    graph_likelihood = np.array(graph_likelihood)\n",
        "\n",
        "    min_pred = np.min(graph_pred)\n",
        "    min_std = np.min(graph_std)\n",
        "    max_likelihood = np.max(graph_likelihood)\n",
        "\n",
        "    pos_min_pred = np.argwhere(graph_pred == np.min(graph_pred))\n",
        "    pos_min_std = np.argwhere(graph_std == np.min(graph_std))\n",
        "    pos_max_likelihood = np.argwhere(graph_likelihood == np.max(graph_likelihood))\n",
        "\n",
        "    graph_x_f = graph_x.flatten()\n",
        "    graph_y_f = graph_y.flatten()\n",
        "    graph_pred_f = graph_pred.flatten()\n",
        "    graph_std_f = graph_std.flatten()\n",
        "    graph_likelihood_f = graph_likelihood.flatten()\n",
        "\n",
        "    graph_pred_f = minmax_scale(graph_pred_f,(5,30))\n",
        "    graph_std_f = minmax_scale(graph_std_f,(5,30))\n",
        "    graph_likelihood_f = minmax_scale(graph_std_f,(5,30))\n",
        "\n",
        "    if eval_metric == 'likelihood':\n",
        "        for i in range(len(graph_likelihood_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_likelihood_f[i], color='orange')\n",
        "\n",
        "        plt.plot(graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], graph_y[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('likelihood w.r.t. different length scale and nu')\n",
        "        plt.show()\n",
        "        print('With likelihood as metric, the maximum likelihood value is', max_likelihood, 'when kernel is Matern with 1d length scale of', graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'and nu of', nu_list[pos_max_likelihood[0][1]])\n",
        "\n",
        "    if eval_metric == 'rmse':\n",
        "        for i in range(len(graph_pred_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_pred_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], graph_y[pos_min_pred[0][0],pos_min_pred[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale and nu on prediction')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on prediction is', min_pred, 'when kernel is Matern with 1d length scale of', graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], 'and nu of', nu_list[pos_min_pred[0][1]])\n",
        "\n",
        "\n",
        "        for i in range(len(graph_std_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_std_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_std[0][0],pos_min_std[0][1]], graph_y[pos_min_std[0][0],pos_min_std[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale and nu on std')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on std is', min_std, 'when kernel is Matern with 1d length scale of', graph_x[pos_min_std[0][0],pos_min_std[0][1]], 'and nu of', nu_list[pos_min_std[0][1]])\n",
        "\n",
        "\n",
        "\n",
        "    #return graph_x,graph_y,graph_pred,graph_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#              rmse,likelihood\n",
        "grid_search_1d_length_scale_nu('likelihood', X_normalized, 0.1, 1.1, 0.1)"
      ],
      "metadata": {
        "id": "Xtv9i_W8cMaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nufloat, default=1.5\n",
        "\n",
        "The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized."
      ],
      "metadata": {
        "id": "_686kByRrk2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D LengthScale Search"
      ],
      "metadata": {
        "id": "k_0odwwfQoCl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Y0T4BdhWFvV6"
      },
      "outputs": [],
      "source": [
        "def change_model_1d_length_scale(X_train, X_test, y_train, y_test, std_test, kernel_name, length_scale, X_normalized, test_results, eval_metric):\n",
        "    # initialize\n",
        "    i = 0\n",
        "    prediction_history = []\n",
        "    std_history = []\n",
        "    X_next = []\n",
        "    X_next_availble = []\n",
        "\n",
        "    if X_normalized:\n",
        "        bounds = np.array([[0,1],[0,1]])     #True\n",
        "    else:\n",
        "        bounds = np.array([[1,4],[15,35]])    #False\n",
        "\n",
        "    if kernel_name == 'RBF':\n",
        "      kernel = 1 * RBF(length_scale)\n",
        "    if kernel_name == 'RQ':\n",
        "      kernel = 1 * RQ(length_scale, alpha = 50) # Change alpha here if using RQ kernel\n",
        "    if kernel_name == 'Matern':\n",
        "      kernel = 1 * Matern(length_scale, nu = 18) # Change nu here if using Matern\n",
        "    if kernel_name == 'Exp':\n",
        "      kernel = Exp(length_scale = 1, periodicity=1)\n",
        "    # GPR\n",
        "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=9)\n",
        "    Acquisition_function = Upper_boundary #Change Acquisition_function between Upper_boundary and expected_improvement\n",
        "\n",
        "    # Train\n",
        "    model, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, X_sample_dn = predict(gpr, prediction_history, std_history, i, X_train[0:4], y_train[0:4], X_train[4:], y_train[4:], X_next, X_next_availble, bounds, X_normalized=X_normalized, All_samples_at_once = 1, Acquisition_function = Acquisition_function)\n",
        "\n",
        "    # Predict\n",
        "    if X_normalized:\n",
        "        X_test = Normalize(X_test)\n",
        "    y_result = gpr.predict(X_test)\n",
        "    output1, output2 = gpr.predict(X_test, return_std=True)\n",
        "    output1 = output1.flatten()\n",
        "    y_prediction = np.maximum(output1,output2)\n",
        "    y_std = np.minimum(output1,output2)\n",
        "\n",
        "\n",
        "    test_results[0]=(X_test)\n",
        "    test_results[1]=(y_prediction)\n",
        "    test_results[2]=(y_std)\n",
        "    print('test_results')\n",
        "    print(test_results)\n",
        "\n",
        "    #Error Calculation\n",
        "    if eval_metric == 'rmse':\n",
        "      error_value_pred = rmse(y_test,y_prediction)\n",
        "      error_value_std = rmse(std_test,y_std)\n",
        "      likelihood_value = 0\n",
        "    if eval_metric == 'likelihood':\n",
        "      error_value_pred = 0\n",
        "      error_value_std = 0\n",
        "      likelihood_value = model.log_marginal_likelihood()\n",
        "\n",
        "    return error_value_pred, error_value_std, likelihood_value, y_std, y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "C8E7ZusmkSzg"
      },
      "outputs": [],
      "source": [
        "def grid_search_1d_length_scale(kernel_name, eval_metric, X_normalized, length_scale_start = 0.1, length_scale_end = 1.1, length_scale_step = 0.1):\n",
        "    graph_x = []\n",
        "    graph_pred = []\n",
        "    graph_std = []\n",
        "    graph_likelihood = []\n",
        "    test_results = [[],[],[]]\n",
        "\n",
        "    for length_scale in np.arange(length_scale_start, length_scale_end, length_scale_step):#调这儿来改grid search的range\n",
        "        print('Running %.4f' %(length_scale))\n",
        "        error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_1d_length_scale(X_train_6, X_test_6, y_train_6, y_test_6, std_test, kernel_name, length_scale, X_normalized, test_results, eval_metric)\n",
        "        if eval_metric == 'likelihood':\n",
        "            print('length_scale is', length_scale, 'likelihood_value is', likelihood_value)\n",
        "        else:\n",
        "            print('length_scale is', length_scale, 'error_value_pred is', error_value_pred, 'error_value_std is', error_value_std)\n",
        "        graph_x.append(length_scale)\n",
        "        graph_pred.append(error_value_pred)\n",
        "        graph_std.append(error_value_std)\n",
        "        graph_likelihood.append(likelihood_value)\n",
        "        print('')\n",
        "    graph_x=np.array(graph_x)\n",
        "    graph_pred=np.array(graph_pred)\n",
        "    graph_std=np.array(graph_std)\n",
        "    graph_likelihood = np.array(graph_likelihood)\n",
        "\n",
        "    min_pred = np.min(graph_pred)\n",
        "    min_std = np.min(graph_std)\n",
        "    max_likelihood = np.max(graph_likelihood)\n",
        "\n",
        "    pos_min_pred = np.argwhere(graph_pred == np.min(graph_pred))[0]\n",
        "    pos_min_std = np.argwhere(graph_std == np.min(graph_std))[0]\n",
        "    pos_max_likelihood = np.argwhere(graph_likelihood == np.max(graph_likelihood))[0]\n",
        "\n",
        "    graph_x_f = graph_x.flatten()\n",
        "    graph_pred_f = graph_pred.flatten()\n",
        "    graph_std_f = graph_std.flatten()\n",
        "    graph_likelihood_f = graph_likelihood.flatten()\n",
        "\n",
        "    graph_pred_f = minmax_scale(graph_pred_f,(5,30))\n",
        "    graph_std_f = minmax_scale(graph_std_f,(5,30))\n",
        "    graph_likelihood_f = minmax_scale(graph_std_f,(5,30))\n",
        "\n",
        "\n",
        "    if eval_metric == 'likelihood':\n",
        "        plt.scatter(graph_x, graph_likelihood, color='orange')\n",
        "        plt.scatter(graph_x[pos_max_likelihood], graph_likelihood[pos_max_likelihood], color='red')\n",
        "        plt.title('likelihood w.r.t. different length scale')\n",
        "        plt.show()\n",
        "        print('With likelihood as metric, the maximum likelihood value is', max_likelihood, 'when kernel is', kernel_name, 'with 1d length scale of', graph_x[pos_max_likelihood[0]])\n",
        "\n",
        "    if eval_metric == 'rmse':\n",
        "        plt.scatter(graph_x, graph_pred, color='orange')\n",
        "        plt.scatter(graph_x[pos_min_pred], graph_pred[pos_min_pred], color='red')\n",
        "        plt.title('rmse w.r.t. different length scale on prediction')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on prediction is', min_pred, 'when kernel is', kernel_name, 'with 1d length scale of', graph_x[pos_min_pred[0]])\n",
        "\n",
        "\n",
        "        plt.scatter(graph_x, graph_std, color='orange')\n",
        "        plt.scatter(graph_x[pos_min_std], graph_std[pos_min_std], color='red')\n",
        "        plt.title('rmse w.r.t. different length scale on std')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on std is', min_std, 'when kernel is', kernel_name, 'with 1d length scale of', graph_x[pos_min_std[0]])\n",
        "\n",
        "\n",
        "    #return graph_x,graph_pred,graph_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF, Matern, RQ;     rmse,likelihood\n",
        "grid_search_1d_length_scale('RBF', 'likelihood', X_normalized, length_scale_start = 0.1, length_scale_end = 1.1, length_scale_step = 0.1)"
      ],
      "metadata": {
        "id": "yHPufM8lXMu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_normalized = True\n",
        "test_results = [[],[],[]]\n",
        "length_scale = 0.4\n",
        "error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_1d_length_scale(X_train_6, X_test_6, y_train_6, y_test_6, std_test, 'RBF', length_scale, X_normalized, test_results, 'likelihood')"
      ],
      "metadata": {
        "id": "mXPY2Plxczfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2D LengthScale Search on Matern(fixed nu = 1.5) and RBF"
      ],
      "metadata": {
        "id": "G5CQK8Qdfyvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pJgkJq6AgMhK"
      },
      "outputs": [],
      "source": [
        "def change_model_2d_length_scale(X_train, X_test, y_train, y_test, std_test, kernel_name, length_scale, X_normalized, test_results, eval_metric):\n",
        "    # initialize\n",
        "    i = 0\n",
        "    prediction_history = []\n",
        "    std_history = []\n",
        "    X_next = []\n",
        "    X_next_availble = []\n",
        "\n",
        "    if X_normalized:\n",
        "        bounds = np.array([[0,1],[0,1]])     #True\n",
        "    else:\n",
        "        bounds = np.array([[1,4],[15,35]])    #False\n",
        "\n",
        "    if kernel_name == 'RBF':\n",
        "      kernel = 1 * RBF(length_scale)\n",
        "    if kernel_name == 'RQ':\n",
        "      kernel = 1 * RQ(length_scale, alpha = 50) # Change alpha here if using RQ kernel\n",
        "    if kernel_name == 'Matern':\n",
        "      kernel = 1 * Matern(length_scale, nu = 18) # Change nu here if using Matern\n",
        "    if kernel_name == 'Exp':\n",
        "      kernel = Exp(length_scale = 1, periodicity=1)\n",
        "    # GPR\n",
        "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=9)\n",
        "    Acquisition_function = Upper_boundary #Change Acquisition_function between Upper_boundary and expected_improvement\n",
        "\n",
        "    # Train\n",
        "    model, prediction_history, std_history, i, X_init, Y_init, X_pool, Y_pool, X_next, X_next_availble, bounds, X_sample_dn = predict(gpr, prediction_history, std_history, i, X_train[0:4], y_train[0:4], X_train[4:], y_train[4:], X_next, X_next_availble, bounds, X_normalized=X_normalized, All_samples_at_once = 1, Acquisition_function = Acquisition_function)\n",
        "\n",
        "    # Predict\n",
        "    if X_normalized:\n",
        "        X_test = Normalize(X_test)\n",
        "    y_result = gpr.predict(X_test)\n",
        "    output1, output2 = gpr.predict(X_test, return_std=True)\n",
        "    output1 = output1.flatten()\n",
        "    y_prediction = np.maximum(output1,output2)\n",
        "    y_std = np.minimum(output1,output2)\n",
        "\n",
        "\n",
        "    test_results[0]=(X_test)\n",
        "    test_results[1]=(y_prediction)\n",
        "    test_results[2]=(y_std)\n",
        "    print('test_results')\n",
        "    print(test_results)\n",
        "\n",
        "    #Error Calculation\n",
        "    if eval_metric == 'rmse':\n",
        "      error_value_pred = rmse(y_test,y_prediction)\n",
        "      error_value_std = rmse(std_test,y_std)\n",
        "      likelihood_value = 0\n",
        "    if eval_metric == 'likelihood':\n",
        "      error_value_pred = 0\n",
        "      error_value_std = 0\n",
        "      likelihood_value = model.log_marginal_likelihood()\n",
        "\n",
        "    return error_value_pred, error_value_std, likelihood_value, y_std, y_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P5xv3VeOgMhL"
      },
      "outputs": [],
      "source": [
        "def grid_search_2d_length_scale(kernel_name, eval_metric, X_normalized, length_scale_0_start = 0.6, length_scale_0_end = 1.1, length_scale_0_step = 0.1, length_scale_1_start = 0.2, length_scale_1_end = 1.1, length_scale_1_step = 0.1):\n",
        "    graph_x = []\n",
        "    graph_y = []\n",
        "    graph_pred = []\n",
        "    graph_std = []\n",
        "    graph_likelihood = []\n",
        "    test_results = [[],[],[]]\n",
        "\n",
        "    for length_scale_0 in np.arange(length_scale_0_start, length_scale_0_end, length_scale_0_step):#调这儿来改grid search的range\n",
        "        graph_x_row = []\n",
        "        graph_y_row = []\n",
        "        graph_z_pred = []\n",
        "        graph_z_std = []\n",
        "        graph_z_likelihood = []\n",
        "\n",
        "        for length_scale_1 in np.arange(length_scale_1_start, length_scale_1_end, length_scale_1_step):\n",
        "            print('Running %.4f %.4f' %(length_scale_0, length_scale_1))\n",
        "            length_scale = [length_scale_0,length_scale_1]\n",
        "            error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_2d_length_scale(X_train_6, X_test_6, y_train_6, y_test_6, std_test, kernel_name, length_scale, X_normalized, test_results, eval_metric)\n",
        "\n",
        "\n",
        "            if eval_metric == 'likelihood':\n",
        "                print('length_scale_0 is', length_scale_0, 'length_scale_1 is', length_scale_1, 'likelihood_value is', likelihood_value)\n",
        "            else:\n",
        "                print('length_scale_0 is', length_scale_0, 'length_scale_1 is', length_scale_1, 'error_value_pred is', error_value_pred, 'error_value_std is', error_value_std)\n",
        "\n",
        "            graph_x_row.append(length_scale_0)\n",
        "            graph_y_row.append(length_scale_1)\n",
        "            graph_z_pred.append(error_value_pred)\n",
        "            graph_z_std.append(error_value_std)\n",
        "            graph_z_likelihood.append(likelihood_value)\n",
        "\n",
        "        graph_x.append(graph_x_row)\n",
        "        graph_y.append(graph_y_row)\n",
        "        graph_pred.append(graph_z_pred)\n",
        "        graph_std.append(graph_z_std)\n",
        "        graph_likelihood.append(graph_z_likelihood)\n",
        "        print('')\n",
        "\n",
        "    graph_x = np.array(graph_x)\n",
        "    graph_y = np.array(graph_y)\n",
        "    graph_pred = np.array(graph_pred)\n",
        "    graph_std = np.array(graph_std)\n",
        "    graph_likelihood = np.array(graph_likelihood)\n",
        "\n",
        "    min_pred = np.min(graph_pred)\n",
        "    min_std = np.min(graph_std)\n",
        "    max_likelihood = np.max(graph_likelihood)\n",
        "\n",
        "    pos_min_pred = np.argwhere(graph_pred == np.min(graph_pred))\n",
        "    pos_min_std = np.argwhere(graph_std == np.min(graph_std))\n",
        "    pos_max_likelihood = np.argwhere(graph_likelihood == np.max(graph_likelihood))\n",
        "\n",
        "    graph_x_f = graph_x.flatten()\n",
        "    graph_y_f = graph_y.flatten()\n",
        "    graph_pred_f = graph_pred.flatten()\n",
        "    graph_std_f = graph_std.flatten()\n",
        "    graph_likelihood_f = graph_likelihood.flatten()\n",
        "\n",
        "    graph_pred_f = minmax_scale(graph_pred_f,(5,30))\n",
        "    graph_std_f = minmax_scale(graph_std_f,(5,30))\n",
        "    graph_likelihood_f = minmax_scale(graph_std_f,(5,30))\n",
        "\n",
        "    if eval_metric == 'likelihood':\n",
        "        for i in range(len(graph_likelihood_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_likelihood_f[i], color='orange')\n",
        "\n",
        "        plt.plot(graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], graph_y[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('likelihood w.r.t. different length scale')\n",
        "        plt.show()\n",
        "        print('With likelihood as metric, the maximum likelihood value is', max_likelihood, 'when kernel is', kernel_name, 'with 2d length scale of', graph_x[pos_max_likelihood[0][0],pos_max_likelihood[0][1]], 'and', graph_y[pos_max_likelihood[0][0],pos_max_likelihood[0][1]])\n",
        "\n",
        "    if eval_metric == 'rmse':\n",
        "        for i in range(len(graph_pred_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_pred_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], graph_y[pos_min_pred[0][0],pos_min_pred[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale on prediction')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on prediction is', min_pred, 'when kernel is', kernel_name, 'with 2d length scale of', graph_x[pos_min_pred[0][0],pos_min_pred[0][1]], 'and', graph_y[pos_min_pred[0][0],pos_min_pred[0][1]])\n",
        "\n",
        "\n",
        "        for i in range(len(graph_std_f)):\n",
        "            #print('ploting {}'.format(i))\n",
        "            plt.plot(graph_x_f[i], graph_y_f[i], 'o', markersize = graph_std_f[i], color='orange');\n",
        "\n",
        "        plt.plot(graph_x[pos_min_std[0][0],pos_min_std[0][1]], graph_y[pos_min_std[0][0],pos_min_std[0][1]], 'o', markersize = 5, color='red');\n",
        "        plt.title('rmse w.r.t. different length scale on std')\n",
        "        plt.show()\n",
        "        print('With rmse as metric, the minimum rmse on std is', min_std, 'when kernel is', kernel_name, 'with 2d length scale of', graph_x[pos_min_std[0][0],pos_min_std[0][1]], 'and', graph_y[pos_min_std[0][0],pos_min_std[0][1]])\n",
        "\n",
        "\n",
        "\n",
        "    #return graph_x,graph_y,graph_pred,graph_std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RBF, Matern, RQ;     rmse,likelihood\n",
        "grid_search_2d_length_scale('Matern', 'rmse', X_normalized, 0.6, 1.1, 0.1, 0.2, 1.1, 0.1)"
      ],
      "metadata": {
        "id": "GtGQmYX4qt9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_normalized = True\n",
        "test_results = [[],[],[]]\n",
        "length_scale = [0.2, 0.7]\n",
        "error_value_pred, error_value_std, likelihood_value, y_std, y_prediction = change_model_2d_length_scale(X_train_6, X_test_6, y_train_6, y_test_6, std_test, 'RBF', length_scale, X_normalized, test_results, 'likelihood')"
      ],
      "metadata": {
        "id": "idXMbVg_VmR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}